# Default values for zitadel.
zitadel:
  # The ZITADEL config under configmapConfig is written to a Kubernetes ConfigMap
  # See all defaults here:
  # https://github.com/zitadel/zitadel/blob/main/cmd/defaults.yaml
  configmapConfig:
    ExternalSecure: true
    ExternalDomain: ""
    TLS:
      Enabled: false
    Database:
      Postgres:
        Host: ""
        Port: 5432
    Machine:
      Identification:
        Hostname:
          Enabled: true
        Webhook:
          Enabled: false
    # Configures the initial instance created by the ZITADEL setup job. The values
    # defined here are used to bootstrap the first organization and its users.
    FirstInstance:
      Skip: false
      # IMPORTANT: The following fields are managed automatically by the Helm chart.
      # Setting these manually is not supported and will cause the deployment to fail.
      # The chart automatically generates and manages these paths internally.
      MachineKeyPath: null
      PatPath: null
      LoginClientPatPath: null
      Org:
        # IMPORTANT: Setting Skip here is not supported. Use FirstInstance.Skip instead.
        # This field exists for validation purposes only.
        Skip: null
        # Defines an administrative machine user created with the IAM_OWNER role.
        # This user is intended for initial automation and administrative tasks
        # right after the Helm chart is deployed.
        #
        # By default, a JWT Machine Key is generated and stored in a Kubernetes
        # secret named after the `Username` (e.g., 'iam-admin').
        #
        # If the `Pat` block below is defined, a Personal Access Token (PAT) is
        # also generated and stored in a separate Kubernetes secret. This secret
        # will be named after the `Username` with a '-pat' suffix (e.g.,
        # 'iam-admin-pat').
        Machine:
          Machine:
            Username: 'iam-admin'
            Name: 'Automatically Initialized IAM Admin'
          MachineKey:
            ExpirationDate: '2029-01-01T00:00:00Z'
            Type: 1
          # If this Pat object is present, a PAT will also be created.
          Pat:
            ExpirationDate: '2029-01-01T00:00:00Z'

        # Defines the internal machine user required by the ZITADEL Login UI.
        # This user communicates with the main ZITADEL API to handle login flows.
        # It is a specialized, internal-use-only account and is not intended for
        # general administrative use. A PAT is generated and stored in a
        # Kubernetes secret named 'login-client'.
        LoginClient:
          Machine:
            Username: 'login-client'
            Name: 'Automatically Initialized IAM Login Client'
          # The expiration date for the Login UI's Personal Access Token (PAT).
          Pat:
            ExpirationDate: '2029-01-01T00:00:00Z'

  # The ZITADEL config under secretConfig is written to a Kubernetes Secret
  # See all defaults here:
  # https://github.com/zitadel/zitadel/blob/main/cmd/defaults.yaml
  secretConfig:

  # Annotations set on secretConfig secret
  secretConfigAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # Reference the name of a secret that contains ZITADEL configuration.
  configSecretName:
  # The key under which the ZITADEL configuration is located in the secret.
  configSecretKey: config-yaml

  # ZITADEL uses the masterkey for symmetric encryption.
  # You can generate it for example with tr -dc A-Za-z0-9 </dev/urandom | head -c 32
  masterkey: ""
  # Reference the name of the secret that contains the masterkey. The key should be named "masterkey".
  # Note: Either zitadel.masterkey or zitadel.masterkeySecretName must be set
  masterkeySecretName: ""

  # Number of old ReplicaSets to retain for rollback purposes
  # Set to 0 to not keep any old ReplicaSets
  revisionHistoryLimit: 10

  # Horizontal Pod Autoscaler configuration for scaling on CPU, memory, or custom metrics.
  # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  autoscaling:
    # If true, enables the Horizontal Pod Autoscaler for the Zitadel deployment.
    # This will automatically override the `replicaCount` value.
    enabled: false
    # Optional map of annotations applied to the HPA object.
    annotations: {}
    # The minimum number of pod replicas.
    minReplicas: 3
    # The maximum number of pod replicas.
    maxReplicas: 10
    # The target average CPU utilization percentage.
    targetCPU: null
    # The target average memory utilization percentage.
    targetMemory: null
    # Advanced scaling based on custom metrics exposed by Zitadel.
    # Zitadel exposes standard Go runtime metrics. To use these for scaling,
    # you MUST have a metrics server (e.g., Prometheus) and a metrics adapter
    # (e.g., prometheus-adapter) running in your cluster.
    # Ref: https://github.com/kubernetes-sigs/prometheus-adapter
    metrics: []
    # Example: Scale when the average number of goroutines per pod exceeds 150.
    # The `go_goroutines` metric is a good proxy for concurrent load.
    # You should observe your application's baseline to find a suitable value.
    # - type: Pods
    #   pods:
    #     metric:
    #       # This name must match the metric exposed by the prometheus-adapter.
    #       name: "go_goroutines"
    #     target:
    #       type: AverageValue
    #       averageValue: "150"
    # Configures the scaling behavior for scaling up and down.
    # See: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#configurable-scaling-behavior
    behavior: {}

  # Annotations set on masterkey secret
  masterkeyAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # The CA Certificate needed for establishing secure database connections
  dbSslCaCrt: ""

  # Annotations set on database SSL CA certificate secret
  dbSslCaCrtAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # The Secret containing the CA certificate at key ca.crt needed for establishing secure database connections
  dbSslCaCrtSecret: ""

  # The db admins secret containing the client certificate and key at tls.crt and tls.key needed for establishing secure database connections
  dbSslAdminCrtSecret: ""

  # The db users secret containing the client certificate and key at tls.crt and tls.key needed for establishing secure database connections
  dbSslUserCrtSecret: ""

  # The Secret containing the certificate at key tls.crt and tls.key for listening on HTTPS
  serverSslCrtSecret: ""

  # Generate a self-signed TLS certificate using Helm's native functions.
  # If enabled, this creates a Kubernetes secret containing a self-signed
  # certificate and mounts it to /etc/tls/ in the ZITADEL pod.
  #
  # This is useful for quick demos or test environments without an ingress
  # controller. It is NOT recommended for production use.
  #
  # The certificate will be valid for the `ExternalDomain`, `localhost`,
  # and any host specified in `additionalDnsName`. It can't use dynamic
  # values like the Pod IP, which the previous initContainer method did.
  selfSignedCert:
    # -- If true, enables the generation of the self-signed certificate.
    enabled: false
    # -- An additional DNS name (SAN) to add to the certificate.
    # e.g., "my-zitadel.example.com"
    additionalDnsName: ""

  # Enabling this will create a debug pod that can be used to inspect the ZITADEL configuration and run zitadel commands using the zitadel binary.
  # This is useful for debugging and troubleshooting.
  # After the debug pod is created, you can open a shell within the pod.
  # See more instructions by printing the pods logs using kubectl logs [pod name].
  debug:
    enabled: false
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-weight: "1"
    initContainers: []
    extraContainers: []

  # initContainers allow you to add any init containers you wish to use globally.
  # Additionally, they follow the same structure as extraContainers
  initContainers: []
  # extraContainers allows you to add any sidecar containers you wish to use globally.
  # Currently this is the Zitadel Deployment, Setup Job**, Init Job** and debug_replicaset**  **If Enabled
  extraContainers: []
    # # Example; You wish to deploy a cloud-sql-proxy sidecar to all deployments:
    # - name: cloud-sql-proxy
    #   image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.14.1
    #   command:
    #     - /cloud-sql-proxy
    #   args:
    #     - my-project:my-region:my-instance
    #     - --port=5432
    #     - --auto-iam-authn
    #     - --health-check
    #     - "--http-address=0.0.0.0"
    #   ports:
    #     - containerPort: 5432
    #   startupProbe:
    #     httpGet:
    #       path: /startup
    #       port: 9090
    #     periodSeconds: 1
    #     timeoutSeconds: 5
    #   livenessProbe:
    #     httpGet:
    #       path: /liveness
    #       port: 9090
    #     initialDelaySeconds: 0
    #     periodSeconds: 60
    #     timeoutSeconds: 30
    #     failureThreshold: 5
    #   securityContext:
    #     runAsNonRoot: true
    #     readOnlyRootFilesystem: true
    #     allowPrivilegeEscalation: false
    #   lifecycle:
    #     postStart:
    #       exec:
    #         command: ["/cloud-sql-proxy", "wait"]

login:
  enabled: true
  # Pod Disruption Budget configuration for the main ZITADEL deployment.
  # Ensures high availability by limiting the number of pods that can be
  # simultaneously unavailable during voluntary disruptions (e.g., node drains,
  # rolling updates). Either minAvailable or maxUnavailable can be set, but not
  # both. Values can be an integer (e.g., 1) or a percentage (e.g., "50%").
  pdb:
    # Enable or disable the Pod Disruption Budget for ZITADEL pods
    enabled: false
    # Minimum number of pods that must remain available during disruptions.
    # Cannot be used together with maxUnavailable.
    minAvailable: null
    # Maximum number of pods that can be unavailable during disruptions.
    # Cannot be used together with minAvailable.
    maxUnavailable: null
    # Additional annotations to apply to the Pod Disruption Budget resource
    annotations: {}

  # customConfigmapConfig defaults to this:
  # ZITADEL_SERVICE_USER_TOKEN_FILE="/login-client/pat"
  # ZITADEL_API_URL="http://{{ include "zitadel.fullname" . }}:{{ .Values.service.port }}"
  # CUSTOM_REQUEST_HEADERS="Host:{{ .Values.zitadel.configmapConfig.ExternalDomain }}"
  customConfigmapConfig:
  # To deploy zitadel multiple times in the same namespace, use a loginClientSecretPrefix.
  # To mount it, also change the referenced secretName for the login client to "{loginClientSecretPrefix}login-client"
  loginClientSecretPrefix:
  extraVolumeMounts: []
  extraVolumes: []
  # Number of pod replicas. While a single replica is fine for testing,
  # production environments should use 3 or more to prevent downtime during
  # updates and ensure high availability.
  replicaCount: 1
  initContainers: []
  extraContainers: []
  image:
    repository: ghcr.io/zitadel/zitadel-login
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  annotations: {}
  podAnnotations: {}
  podAdditionalLabels: {}
  # Annotations to add to the configMap
  configMap:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "0"
  readinessProbe:
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 5
    failureThreshold: 3
  livenessProbe:
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 5
    failureThreshold: 3
  startupProbe:
    enabled: false
    periodSeconds: 1
    failureThreshold: 30
  service:
    type: ClusterIP
    # If service type is "ClusterIP", this can optionally be set to a fixed IP address.
    clusterIP: ""
    # If service type is "LoadBalancer", this can optionally be set to either "Cluster" or "Local"
    externalTrafficPolicy: ""
    port: 3000
    protocol: http
    appProtocol: kubernetes.io/http
    annotations: {}
    labels: {}
    scheme: HTTP
  # Ingress configuration for exposing the ZITADEL Login UI service. This makes
  # the web-based login interface accessible from outside the cluster. It can be
  # configured independently of the main ZITADEL service Ingress.
  ingress:
    # If true, creates an Ingress resource for the Login UI service.
    enabled: false
    # The name of the IngressClass resource to use for this Ingress.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class
    className: ""
    # A chart-specific setting to enable logic for different controllers.
    # Use "aws" to generate AWS ALB-specific annotations.
    controller: generic
    # A map of annotations to apply to the Login UI Ingress resource.
    annotations: {}
    # A list of host rules for the Ingress. The default path targets the login UI.
    hosts:
      - paths:
          - path: /ui/v2/login
            pathType: Prefix
    # TLS configuration for the Ingress. Secure the login UI with HTTPS by
    # referencing a secret containing the TLS certificate and key.
    tls: []
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "0"
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    readOnlyRootFilesystem: true
    privileged: false
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []
  # Additional environment variables
  env:
    []
    # - name: ZITADEL_DATABASE_POSTGRES_HOST
    #   valueFrom:
    #     secretKeyRef:
    #       name: postgres-pguser-postgres
    #       key: host
  revisionHistoryLimit: 10

  # Horizontal Pod Autoscaler configuration for scaling on CPU, memory, or custom metrics.
  # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  autoscaling:
    # If true, enables the Horizontal Pod Autoscaler for the login deployment.
    # This will automatically override the `replicaCount` value.
    enabled: false
    # Optional map of annotations applied to the HPA object.
    annotations: {}
    # The minimum number of pod replicas.
    minReplicas: 3
    # The maximum number of pod replicas.
    maxReplicas: 10
    # The target average CPU utilization percentage.
    targetCPU: null
    # The target average memory utilization percentage.
    targetMemory: null
    # Advanced scaling based on custom metrics exposed by Zitadel.
    # To use these for scaling, you MUST have a metrics server
    # (e.g., Prometheus) and a metrics adapter (e.g., prometheus-adapter)
    # running in your cluster.
    # Ref: https://github.com/kubernetes-sigs/prometheus-adapter
    metrics: []
    behavior: {}

# Number of pod replicas. While a single replica is fine for testing,
# production environments should use 3 or more to prevent downtime during
# updates and ensure high availability.
replicaCount: 1

image:
  repository: ghcr.io/zitadel/zitadel
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Annotations to add to the deployment
annotations: {}

# Annotations to add to the configMap
configMap:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podAdditionalLabels: {}

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  readOnlyRootFilesystem: true
  privileged: false

# Additional environment variables
env:
  []
  # - name: ZITADEL_DATABASE_POSTGRES_HOST
  #   valueFrom:
  #     secretKeyRef:
  #       name: postgres-pguser-postgres
  #       key: host

# Additional environment variables from the given secret name
# Zitadel can be configured using environment variables from a secret.
# Reference: https://zitadel.com/docs/self-hosting/manage/configure#configure-by-environment-variables
envVarsSecret: ""

service:
  type: ClusterIP
  # If service type is "ClusterIP", this can optionally be set to a fixed IP address.
  clusterIP: ""
  # If service type is "LoadBalancer", this can optionally be set to either "Cluster" or "Local"
  externalTrafficPolicy: ""
  port: 8080
  protocol: http2
  appProtocol: kubernetes.io/h2c
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels: {}
  scheme: HTTP

# Ingress configuration for exposing the main ZITADEL service. This allows
# external traffic to reach the ZITADEL API and gRPC endpoints. It can be
# configured to work with various ingress controllers like NGINX, Traefik, or
# AWS ALB.
ingress:
  # If true, creates an Ingress resource for the ZITADEL service.
  enabled: false
  # A chart-specific setting to enable logic for different controllers.
  # Use "aws" to generate AWS ALB-specific annotations and resources.
  controller: generic
  # The name of the IngressClass resource to use for this Ingress.
  # Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class
  className: ""
  # A map of annotations to apply to the Ingress resource. The default annotation
  # is for NGINX to correctly handle gRPC traffic.
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
  # A list of host rules for the Ingress. Each host can have multiple paths.
  hosts:
    - paths:
        - path: /
          pathType: Prefix
  # TLS configuration for the Ingress. This allows you to secure the endpoint
  # with HTTPS by referencing a secret that contains the TLS certificate and key.
  tls: []

resources: {}

nodeSelector: {}

tolerations: []

affinity: {}

topologySpreadConstraints: []

initJob:
  # Once ZITADEL is installed, the initJob can be disabled.
  enabled: true
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "1"
  resources: {}
  backoffLimit: 5
  activeDeadlineSeconds: 300
  initContainers: []
  extraContainers: []
  podAnnotations: {}
  podAdditionalLabels: {}
  # Available init commands :
  # "": initialize ZITADEL instance (without skip anything)
  # database: initialize only the database
  # grant: set ALL grant to user
  # user: initialize only the database user
  # zitadel: initialize ZITADEL internals (skip "create user" and "create database")
  command: ""

# Cleanup Job configuration for removing imperatively created resources on
# helm uninstall. The setup job creates secrets using kubectl that aren't
# tracked by Helm's lifecycle, causing them to persist after chart removal.
# This job runs as a post-delete hook to clean up those orphaned resources.
cleanupJob:
  # Enable the cleanup job to remove secrets created by the setup job.
  # Set to false if you want to preserve secrets across reinstalls.
  enabled: true
  # Annotations for the cleanup job. The post-delete hook ensures this runs
  # on helm uninstall, and the delete policy removes the job after completion.
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-1"
  # Number of retries before marking the cleanup job as failed.
  backoffLimit: 3
  # Maximum time in seconds for the cleanup job to complete.
  # After this deadline, the job is terminated even if still running.
  activeDeadlineSeconds: 60
  # Resource limits and requests for the cleanup job container.
  # Keep minimal as this job only runs kubectl delete commands.
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 50m
    #   memory: 64Mi
  # Additional annotations to add to cleanup job pods.
  podAnnotations: {}
  # Additional labels to add to cleanup job pods.
  podAdditionalLabels: {}

setupJob:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "2"
  resources: {}
  backoffLimit: 5
  activeDeadlineSeconds: 300
  initContainers: []
  extraContainers: []
  podAnnotations: {}
  podAdditionalLabels: {}
  additionalArgs:
    - "--init-projections=true"
  machinekeyWriter:
    image:
      repository: alpine/k8s
      tag: ""
    resources: {}

readinessProbe:
  enabled: true
  initialDelaySeconds: 0
  periodSeconds: 5
  failureThreshold: 3

livenessProbe:
  enabled: true
  initialDelaySeconds: 0
  periodSeconds: 5
  failureThreshold: 3

startupProbe:
  enabled: true
  periodSeconds: 1
  failureThreshold: 30

metrics:
  enabled: false
  serviceMonitor:
    # If true, the chart creates a ServiceMonitor that is compatible with Prometheus Operator
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.ServiceMonitor.
    # The Prometheus community Helm chart installs this operator
    # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#kube-prometheus-stack
    # Install the prometheus Helm release with the value prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
    # In the Prometheus UI, query Zitadel metrics, for example:
    # sum({__name__="projection_events_processed_total"})
    enabled: false
    honorLabels: false
    honorTimestamps: true
    namespace: null
    additionalLabels: {}
    scrapeInterval: null
    scrapeTimeout: null
    relabellings: []
    metricRelabellings: []
    scheme: null
    tlsConfig: {}
    proxyUrl: null

# Pod Disruption Budget configuration for the main ZITADEL deployment.
# Ensures high availability by limiting the number of pods that can be
# simultaneously unavailable during voluntary disruptions (e.g., node drains,
# rolling updates). Either minAvailable or maxUnavailable can be set, but not
# both. Values can be an integer (e.g., 1) or a percentage (e.g., "50%").
pdb:
  # Enable or disable the Pod Disruption Budget for ZITADEL pods
  enabled: false
  # Minimum number of pods that must remain available during disruptions.
  # Cannot be used together with maxUnavailable.
  minAvailable: null
  # Maximum number of pods that can be unavailable during disruptions.
  # Cannot be used together with minAvailable.
  maxUnavailable: null
  # Additional annotations to apply to the Pod Disruption Budget resource
  annotations: {}

# extraContainers allows you to add any sidecar containers you wish to use in the Zitadel pod.
extraContainers: []

extraVolumes: []
  # - name: ca-certs
  #   secret:
  #     defaultMode: 420
  #     secretName: ca-certs

extraVolumeMounts: []
  # - name: ca-certs
  #   mountPath: /etc/ssl/certs/myca.pem
  #   subPath: myca.pem
  #   readOnly: true

# extraManifests allows you to add your own Kubernetes manifests
# You can use templating logic like {{ .Release.Namespace }} and {{ .Values.replicaCount }} as long as your manifest is a valid YAML
extraManifests: []
  # - apiVersion: v1
  #   kind: Secret
  #   metadata:
  #     name: {{ include "zitadel.fullname" . }}-my-secret
  #   stringData:
  #     key: value
  #   type: Opaque
