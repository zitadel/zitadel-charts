# Default values for the ZITADEL Helm chart.
# This file contains all configurable parameters. For detailed documentation
# on each parameter, see the inline comments below or visit:
# https://zitadel.com/docs/self-hosting/deploy/kubernetes

zitadel:
  # -- (object) ZITADEL runtime configuration written to a Kubernetes ConfigMap. These
  # values are passed directly to the ZITADEL binary and control its behavior. For the
  # complete list of available configuration options, see:
  # https://github.com/zitadel/zitadel/blob/main/cmd/defaults.yaml
  configmapConfig:
    # Whether external connections use HTTPS. Set to true when ZITADEL is behind
    # a TLS-terminating proxy or ingress controller. This affects URL generation
    # for redirects, OIDC discovery, and other external-facing endpoints.
    ExternalSecure: true
    # The domain name used for external access to ZITADEL (e.g., "auth.example.com").
    # This is used for OIDC issuer URLs, redirect URIs, and cookie domains. Must
    # match the domain configured in your ingress or load balancer.
    ExternalDomain: ""
    # Internal TLS configuration for ZITADEL's HTTP server. When enabled, ZITADEL
    # serves HTTPS directly instead of relying on a proxy for TLS termination.
    TLS:
      # Enable HTTPS on ZITADEL's internal server. Requires serverSslCrtSecret or
      # selfSignedCert to be configured with valid certificates.
      Enabled: false
    # Database connection configuration. ZITADEL requires PostgreSQL 14+ or
    # CockroachDB 22+ as its backing database.
    Database:
      Postgres:
        # PostgreSQL server hostname or IP address. Leave empty if providing via
        # configSecretName or environment variables.
        Host: ""
        # PostgreSQL server port number.
        Port: 5432
    # Machine identification settings for ZITADEL instances in a cluster.
    Machine:
      Identification:
        # Use the pod hostname for machine identification. Recommended for
        # Kubernetes deployments where each pod has a unique hostname.
        Hostname:
          Enabled: true
        # Use a webhook for machine identification. Alternative to hostname-based
        # identification for specialized deployment scenarios.
        Webhook:
          Enabled: false
    # Configures the initial instance created by the ZITADEL setup job. The values
    # defined here are used to bootstrap the first organization and its users.
    FirstInstance:
      Skip: false
      # IMPORTANT: The following fields are managed automatically by the Helm chart.
      # Setting these manually is not supported and will cause the deployment to fail.
      # The chart automatically generates and manages these paths internally.
      MachineKeyPath: null  # @schema type:[null,string]
      PatPath: null  # @schema type:[null,string]
      LoginClientPatPath: null  # @schema type:[null,string]
      Org:
        # IMPORTANT: Setting Skip here is not supported. Use FirstInstance.Skip instead.
        # This field exists for validation purposes only.
        Skip: null  # @schema type:[null,boolean]
        # Defines an administrative machine user created with the IAM_OWNER role.
        # This user is intended for initial automation and administrative tasks
        # right after the Helm chart is deployed.
        #
        # By default, a JWT Machine Key is generated and stored in a Kubernetes
        # secret named after the `Username` (e.g., 'iam-admin').
        #
        # If the `Pat` block below is defined, a Personal Access Token (PAT) is
        # also generated and stored in a separate Kubernetes secret. This secret
        # will be named after the `Username` with a '-pat' suffix (e.g.,
        # 'iam-admin-pat').
        Machine:
          Machine:
            Username: 'iam-admin'
            Name: 'Automatically Initialized IAM Admin'
          MachineKey:
            ExpirationDate: '2029-01-01T00:00:00Z'
            Type: 1
          # If this Pat object is present, a PAT will also be created.
          Pat:
            ExpirationDate: '2029-01-01T00:00:00Z'

        # Defines the internal machine user required by the ZITADEL Login UI.
        # This user communicates with the main ZITADEL API to handle login flows.
        # It is a specialized, internal-use-only account and is not intended for
        # general administrative use. A PAT is generated and stored in a
        # Kubernetes secret named 'login-client'.
        LoginClient:
          Machine:
            Username: 'login-client'
            Name: 'Automatically Initialized IAM Login Client'
          # The expiration date for the Login UI's Personal Access Token (PAT).
          Pat:
            ExpirationDate: '2029-01-01T00:00:00Z'

  # @schema type:[null,object]
  # -- Sensitive ZITADEL configuration values written to a Kubernetes Secret instead
  # of a ConfigMap. Use this for database passwords, API keys, SMTP credentials,
  # and other values that should not be stored in plain text. The secret is
  # mounted alongside the ConfigMap and both are merged by ZITADEL at startup.
  # Structure follows the same format as configmapConfig. See all options:
  # https://github.com/zitadel/zitadel/blob/main/cmd/defaults.yaml
  # Example:
  #   secretConfig:
  #     Database:
  #       Postgres:
  #         User:
  #           Password: "my-secure-password"
  secretConfig:

  # -- (map[string]string) Annotations for the secretConfig Secret. The default Helm hooks
  # ensure the secret is created before the deployment and recreated on upgrades.
  secretConfigAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # @schema type:[null,string]
  # -- Name of an existing Kubernetes Secret containing ZITADEL configuration.
  # Use this when you want to manage ZITADEL configuration externally
  # (e.g., via External Secrets Operator, Sealed Secrets, or GitOps). The secret
  # should contain YAML configuration in the same format as configmapConfig.
  configSecretName:
  # -- The key within the configSecretName secret that contains the ZITADEL
  # configuration YAML. The default "config-yaml" matches the expected format.
  configSecretKey: config-yaml

  # -- ZITADEL's masterkey for symmetric encryption of sensitive data like
  # private keys and tokens. Must be exactly 32 characters (256 bits).
  # Generate with: tr -dc A-Za-z0-9 </dev/urandom | head -c 32
  # IMPORTANT: Store this value securely. Loss of the masterkey means loss of
  # all encrypted data. Either set this value or use masterkeySecretName.
  masterkey: ""
  # -- Name of an existing Kubernetes Secret containing the masterkey at key
  # "masterkey". Use this for production deployments to avoid storing the
  # masterkey in values files. The secret must exist before chart installation.
  # Note: Either zitadel.masterkey or zitadel.masterkeySecretName must be set.
  masterkeySecretName: ""

  # -- Number of old ReplicaSets to retain for rollback purposes
  # Set to 0 to not keep any old ReplicaSets
  revisionHistoryLimit: 10

  # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.PodSecurityContext
  # -- (PodSecurityContext) Optional overrides for the pod security context used by Zitadel pods.
  # If left empty, the chart-wide podSecurityContext defined below is used.
  podSecurityContext: {}
  # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.SecurityContext
  # -- (SecurityContext) Optional overrides for the container security context used by Zitadel pods.
  # If left empty, the chart-wide securityContext defined below is used.
  securityContext: {}

  # Horizontal Pod Autoscaler configuration for scaling on CPU, memory, or custom metrics.
  # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  autoscaling:
    # -- If true, enables the Horizontal Pod Autoscaler for the Zitadel deployment.
    # This will automatically override the `replicaCount` value.
    enabled: false
    # -- (map[string]string) Annotations applied to the HPA object.
    annotations: {}
    # -- The minimum number of pod replicas.
    minReplicas: 3
    # -- The maximum number of pod replicas.
    maxReplicas: 10
    # @schema type:[null,integer]
    # -- The target average CPU utilization percentage.
    targetCPU: null
    # @schema type:[null,integer]
    # -- The target average memory utilization percentage.
    targetMemory: null
    # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.autoscaling.v2.MetricSpec
    # -- ([]MetricSpec) Advanced scaling based on custom metrics exposed by Zitadel.
    # Zitadel exposes standard Go runtime metrics. To use these for scaling,
    # you MUST have a metrics server (e.g., Prometheus) and a metrics adapter
    # (e.g., prometheus-adapter) running in your cluster.
    # Ref: https://github.com/kubernetes-sigs/prometheus-adapter
    metrics: []
    # Example: Scale when the average number of goroutines per pod exceeds 150.
    # The `go_goroutines` metric is a good proxy for concurrent load.
    # You should observe your application's baseline to find a suitable value.
    # - type: Pods
    #   pods:
    #     metric:
    #       # This name must match the metric exposed by the prometheus-adapter.
    #       name: "go_goroutines"
    #     target:
    #       type: AverageValue
    #       averageValue: "150"
    # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.autoscaling.v2.HorizontalPodAutoscalerBehavior
    # -- (HorizontalPodAutoscalerBehavior) Configures the scaling behavior for scaling up and down.
    # See: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#configurable-scaling-behavior
    behavior: {}

  # -- (map[string]string) Annotations for the masterkey Secret when created from
  # zitadel.masterkey. The secret is created once on install and is immutable.
  masterkeyAnnotations:
    helm.sh/hook: pre-install
    helm.sh/hook-weight: "0"

  # -- PEM-encoded CA certificate for verifying the database server's TLS certificate.
  # Use this when your PostgreSQL/CockroachDB server uses a self-signed certificate
  # or a certificate signed by a private CA. The certificate is stored in a
  # Kubernetes Secret and mounted into ZITADEL pods at /db-ssl-ca-crt/ca.crt.
  # Either provide the certificate inline here, or reference an existing secret
  # using dbSslCaCrtSecret instead.
  dbSslCaCrt: ""

  # -- (map[string]string) Annotations for the dbSslCaCrt Secret when created from the
  # inline certificate. The default Helm hooks ensure the secret exists before pods start.
  dbSslCaCrtAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # -- Name of an existing Kubernetes Secret containing the database CA certificate
  # at key "ca.crt". Use this instead of dbSslCaCrt when the certificate is
  # managed externally (e.g., by cert-manager or an operator). The secret must
  # exist in the same namespace as the ZITADEL release.
  dbSslCaCrtSecret: ""

  # -- Name of a Kubernetes Secret containing the admin user's client certificate
  # for mutual TLS (mTLS) authentication to the database. The secret must contain
  # keys "tls.crt" (certificate) and "tls.key" (private key). Used by the init job
  # for database setup operations that require elevated privileges.
  dbSslAdminCrtSecret: ""

  # -- Name of a Kubernetes Secret containing the application user's client certificate
  # for mutual TLS (mTLS) authentication to the database. The secret must contain
  # keys "tls.crt" (certificate) and "tls.key" (private key). Used by the main
  # ZITADEL deployment and setup job for normal database operations.
  dbSslUserCrtSecret: ""

  # -- Name of a Kubernetes Secret containing the TLS certificate for ZITADEL's
  # internal HTTPS server. The secret must contain keys "tls.crt" (certificate)
  # and "tls.key" (private key). Use this when ZITADEL should serve HTTPS directly
  # instead of relying on TLS termination at an ingress controller or load balancer.
  # Requires configmapConfig.TLS.Enabled to be true.
  serverSslCrtSecret: ""

  # Generate a self-signed TLS certificate using Helm's native functions.
  # If enabled, this creates a Kubernetes secret containing a self-signed
  # certificate and mounts it to /etc/tls/ in the ZITADEL pod.
  #
  # This is useful for quick demos or test environments without an ingress
  # controller. It is NOT recommended for production use.
  #
  # The certificate will be valid for the `ExternalDomain`, `localhost`,
  # and any host specified in `additionalDnsName`. It can't use dynamic
  # values like the Pod IP, which the previous initContainer method did.
  selfSignedCert:
    # -- Enable generation of a self-signed TLS certificate.
    enabled: false
    # An additional DNS name (SAN) to include in the certificate.
    # Example: "my-zitadel.example.com"
    additionalDnsName: ""

  # Debug pod configuration. When enabled, creates a standalone pod with the
  # ZITADEL binary and configuration mounted, useful for troubleshooting and
  # running ad-hoc ZITADEL commands. The pod starts in sleep mode, allowing you
  # to exec into it and run commands interactively.
  # Usage: kubectl exec -it <debug-pod-name> -- /bin/sh
  # Then run: zitadel --help
  debug:
    # -- Enable or disable the debug pod. Only enable for troubleshooting; disable
    # in production environments.
    enabled: false
    # -- (map[string]string) Annotations for the debug pod. The Helm hooks ensure it's
    # created during install/upgrade and cleaned up appropriately.
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-weight: "1"
    # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
    # -- ([]Container) Init containers to run before the debug container starts.
    initContainers: []
    # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
    # -- ([]Container) Sidecar containers to run alongside the debug container.
    extraContainers: []

  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # -- ([]Container) Global init containers added to all ZITADEL workloads (Deployment, init job,
  # setup job, and debug pod when enabled). Use this for shared dependencies like
  # database readiness checks or certificate initialization that all workloads need.
  initContainers: []
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # -- ([]Container) Global sidecar containers added to all ZITADEL workloads (Deployment, init job,
  # setup job, and debug pod when enabled). Use this for shared services like
  # database proxies (e.g., cloud-sql-proxy) that all workloads need to connect
  # to the database.
  extraContainers: []
    # # Example; You wish to deploy a cloud-sql-proxy sidecar to all deployments:
    # - name: cloud-sql-proxy
    #   image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.14.1
    #   command:
    #     - /cloud-sql-proxy
    #   args:
    #     - my-project:my-region:my-instance
    #     - --port=5432
    #     - --auto-iam-authn
    #     - --health-check
    #     - "--http-address=0.0.0.0"
    #   ports:
    #     - containerPort: 5432
    #   startupProbe:
    #     httpGet:
    #       path: /startup
    #       port: 9090
    #     periodSeconds: 1
    #     timeoutSeconds: 5
    #   livenessProbe:
    #     httpGet:
    #       path: /liveness
    #       port: 9090
    #     initialDelaySeconds: 0
    #     periodSeconds: 60
    #     timeoutSeconds: 30
    #     failureThreshold: 5
    #   securityContext:
    #     runAsNonRoot: true
    #     readOnlyRootFilesystem: true
    #     allowPrivilegeEscalation: false
    #   lifecycle:
    #     postStart:
    #       exec:
    #         command: ["/cloud-sql-proxy", "wait"]

# Configuration for the ZITADEL Login UI, a separate Next.js application that
# provides the user-facing authentication interface. When enabled, it deploys
# alongside the main ZITADEL service and handles login, registration, and
# password reset flows.
login:
  # -- Enable or disable the Login UI deployment. When disabled, ZITADEL uses its
  # built-in login interface instead of the separate Login UI application.
  enabled: true
  # Pod Disruption Budget configuration for the Login UI deployment.
  # Ensures high availability by limiting the number of pods that can be
  # simultaneously unavailable during voluntary disruptions (e.g., node drains,
  # rolling updates). Either minAvailable or maxUnavailable can be set, but not
  # both. Values can be an integer (e.g., 1) or a percentage (e.g., "50%").
  pdb:
    # -- Enable or disable the Pod Disruption Budget for Login UI pods.
    enabled: false
    # @schema type:[null,integer,string]
    # -- Minimum number of pods that must remain available during disruptions.
    # Cannot be used together with maxUnavailable.
    minAvailable: null
    # @schema type:[null,integer,string]
    # -- Maximum number of pods that can be unavailable during disruptions.
    # Cannot be used together with minAvailable.
    maxUnavailable: null
    # -- (map[string]string) Additional annotations to apply to the Pod Disruption Budget resource.
    annotations: {}

  # @schema type:[null,string]
  # -- Custom environment variables for the Login UI ConfigMap. These override the
  # default values which configure the service user token path, API URL, and
  # custom request headers. Only set this if you need to customize the Login UI
  # behavior beyond the defaults. The defaults are:
  #   ZITADEL_SERVICE_USER_TOKEN_FILE="/login-client/pat"
  #   ZITADEL_API_URL="http://<release>-zitadel:<port>"
  #   CUSTOM_REQUEST_HEADERS="Host:<ExternalDomain>"
  customConfigmapConfig:
  # @schema type:[null,string]
  # -- Prefix for the login client secret name. Use this when deploying multiple
  # ZITADEL instances in the same namespace to avoid secret name collisions.
  # When set, the login client secret will be named "{prefix}login-client".
  loginClientSecretPrefix:
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.VolumeMount
  # -- ([]VolumeMount) Additional volume mounts for the Login UI container. Use this to mount
  # custom certificates, configuration files, or other data into the container.
  extraVolumeMounts: []
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Volume
  # -- ([]Volume) Additional volumes for the Login UI pod. Define volumes here that are
  # referenced by extraVolumeMounts.
  extraVolumes: []
  # -- Number of Login UI pod replicas. A single replica is fine for testing, but
  # production environments should use 3 or more for high availability during
  # rolling updates and node failures.
  replicaCount: 1
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # -- ([]Container) Init containers to run before the Login UI container starts. Useful for
  # waiting on dependencies or performing setup tasks.
  initContainers: []
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # -- ([]Container) Sidecar containers to run alongside the Login UI container. Useful for
  # logging agents, proxies, or other supporting services.
  extraContainers: []
  # Container image configuration for the Login UI.
  image:
    # -- Docker image repository for the Login UI.
    repository: ghcr.io/zitadel/zitadel-login
    # -- Image pull policy. Use "Always" for mutable tags like "latest", or
    # "IfNotPresent" for immutable tags.
    pullPolicy: IfNotPresent
    # -- Image tag. Defaults to the chart's appVersion if not specified. Use a
    # specific version tag for production deployments to ensure reproducibility.
    tag: ""
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.LocalObjectReference
  # -- ([]LocalObjectReference) References to secrets containing Docker registry credentials
  # for pulling private images. Each entry should be the name of an existing secret.
  imagePullSecrets: []
  # -- Override the "login" portion of resource names. Useful when the default
  # naming conflicts with existing resources.
  nameOverride: ""
  # -- Completely override the generated resource names. Takes precedence over
  # nameOverride when set.
  fullnameOverride: ""
  # -- (map[string]string) Annotations to add to the Login UI Deployment resource.
  annotations: {}
  # -- (map[string]string) Annotations to add to Login UI pods. Useful for integrations
  # like Prometheus scraping, Istio sidecar injection, or Vault agent injection.
  podAnnotations: {}
  # -- (map[string]string) Additional labels to add to Login UI pods beyond the standard
  # Helm labels. Useful for organizing pods with custom label selectors.
  podAdditionalLabels: {}
  # ConfigMap configuration for the Login UI environment variables.
  configMap:
    # -- (map[string]string) Annotations for the Login UI ConfigMap. The default hooks
    # ensure the ConfigMap is created before the deployment and recreated on upgrades.
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "0"
  # Readiness probe configuration. The readiness probe determines when a pod is
  # ready to receive traffic. Failed probes remove the pod from service endpoints.
  readinessProbe:
    # -- Enable or disable the readiness probe.
    enabled: true
    # -- Seconds to wait before starting readiness checks after container start.
    initialDelaySeconds: 0
    # -- How often (in seconds) to perform the readiness check.
    periodSeconds: 5
    # -- Number of consecutive failures before marking the pod as not ready.
    failureThreshold: 3
  # Liveness probe configuration. The liveness probe determines if a container
  # is still running. Failed probes cause the container to be restarted.
  livenessProbe:
    # -- Enable or disable the liveness probe.
    enabled: true
    # -- Seconds to wait before starting liveness checks after container start.
    initialDelaySeconds: 0
    # -- How often (in seconds) to perform the liveness check.
    periodSeconds: 5
    # -- Number of consecutive failures before restarting the container.
    failureThreshold: 3
  # Startup probe configuration. The startup probe runs before liveness and
  # readiness probes, allowing slow-starting containers to initialize.
  startupProbe:
    # -- Enable or disable the startup probe. When enabled, liveness and readiness
    # probes are disabled until the startup probe succeeds.
    enabled: false
    # -- How often (in seconds) to perform the startup check.
    periodSeconds: 1
    # -- Number of consecutive failures before marking startup as failed and
    # restarting the container.
    failureThreshold: 30
  # Kubernetes Service configuration for the Login UI.
  service:
    # -- Service type. Use "ClusterIP" for internal access, "NodePort" for node-level
    # access, or "LoadBalancer" for cloud provider load balancers.
    type: ClusterIP
    # -- Fixed cluster IP address for ClusterIP services. Leave empty for automatic
    # assignment. Only applicable when type is "ClusterIP".
    clusterIP: ""
    # -- Traffic policy for LoadBalancer services. "Cluster" distributes traffic to
    # all nodes, "Local" only routes to nodes with pods. Only applicable when
    # type is "LoadBalancer".
    externalTrafficPolicy: ""
    # -- Port number the service exposes. Clients connect to this port.
    port: 3000
    # -- Protocol identifier used in port naming (e.g., "http", "https", "grpc").
    protocol: http
    # -- Application protocol hint for ingress controllers and service meshes.
    # Helps with protocol detection and routing decisions.
    appProtocol: kubernetes.io/http
    # -- (map[string]string) Annotations to add to the Service resource.
    annotations: {}
    # -- (map[string]string) Labels to add to the Service resource.
    labels: {}
    # -- HTTP scheme for health checks and internal communication.
    scheme: HTTP
  # Ingress configuration for exposing the ZITADEL Login UI service. This makes
  # the web-based login interface accessible from outside the cluster. It can be
  # configured independently of the main ZITADEL service Ingress.
  ingress:
    # -- If true, creates an Ingress resource for the Login UI service.
    enabled: false
    # -- The name of the IngressClass resource to use for this Ingress.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class
    className: ""
    # -- A chart-specific setting to enable logic for different controllers.
    # Use "aws" to generate AWS ALB-specific annotations.
    controller: generic
    # -- (map[string]string) Annotations to apply to the Login UI Ingress resource.
    annotations: {}
    # @schema
    # type: array
    # items:
    #   type: object
    #   properties:
    #     paths:
    #       type: array
    #       items:
    #         type: object
    #         properties:
    #           path:
    #             type: string
    #           pathType:
    #             type: string
    #             enum: [Exact, Prefix, ImplementationSpecific]
    #         required: [pathType]
    # @schema
    # -- A list of host rules for the Ingress. The default path targets the login UI.
    hosts:
      - paths:
          - path: /ui/v2/login
            pathType: Prefix
    # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.networking.v1.IngressTLS
    # -- ([]IngressTLS) TLS configuration for the Ingress. Secure the login UI with HTTPS by
    # referencing a secret containing the TLS certificate and key.
    tls: []
  # ServiceAccount configuration for Login UI pods.
  serviceAccount:
    # -- Whether to create a dedicated service account for the Login UI. Set to
    # false to use an existing service account or the default account.
    create: true
    # -- (map[string]string) Annotations for the Login UI service account. The default Helm
    # hooks ensure it exists before pods are created. Add annotations here for cloud
    # provider integrations (e.g., AWS IAM roles, GCP Workload Identity).
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "0"
    # -- The name of the service account to use. If not set and create is true,
    # a name is generated using the fullname template.
    name: ""
  # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.PodSecurityContext
  # -- (PodSecurityContext) Optional pod-level security context overrides for Login UI pods.
  # If left empty, the chart-wide podSecurityContext defined below is used instead.
  # Use this to customize security settings specifically for the Login UI.
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  podSecurityContext: {}
  # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.SecurityContext
  # -- (SecurityContext) Optional container-level security context overrides for the Login UI
  # container. If left empty, the chart-wide securityContext defined below is
  # used instead. Use this to customize security settings specifically for the
  # Login UI container.
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  securityContext: {}
  # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.ResourceRequirements
  # -- (ResourceRequirements) CPU and memory resource requests and limits for the Login UI container.
  # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
    # limits:
    #   cpu: 500m
    #   memory: 512Mi
  # -- (map[string]string) Node labels for pod assignment. Pods will only be scheduled
  # on nodes with matching labels.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  nodeSelector: {}
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Toleration
  # -- ([]Toleration) Tolerations allow pods to be scheduled on nodes with matching taints.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: []
  # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Affinity
  # -- (Affinity) Affinity rules for pod scheduling. Use for advanced pod placement
  # strategies like co-locating pods or spreading across zones.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.TopologySpreadConstraint
  # -- ([]TopologySpreadConstraint) Topology spread constraints control how pods are
  # distributed across topology domains (e.g., zones, nodes) for high availability.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
  topologySpreadConstraints: []
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.EnvVar
  # -- ([]EnvVar) Additional environment variables for the Login UI container. Use this to
  # pass configuration that isn't available through customConfigmapConfig.
  env: []
    # Example: Override the ZITADEL API URL
    # - name: ZITADEL_API_URL
    #   value: "http://zitadel-api:8080"
    # Example: Set a custom header from an existing secret
    # - name: CUSTOM_REQUEST_HEADERS
    #   valueFrom:
    #     secretKeyRef:
    #       name: my-headers-secret
    #       key: headers
  # -- Number of old ReplicaSets to retain for rollback purposes. Set to 0 to
  # disable rollback capability and save cluster resources.
  revisionHistoryLimit: 10

  # Horizontal Pod Autoscaler configuration for scaling on CPU, memory, or custom metrics.
  # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  autoscaling:
    # -- If true, enables the Horizontal Pod Autoscaler for the login deployment.
    # This will automatically override the `replicaCount` value.
    enabled: false
    # -- (map[string]string) Annotations applied to the HPA object.
    annotations: {}
    # -- The minimum number of pod replicas.
    minReplicas: 3
    # -- The maximum number of pod replicas.
    maxReplicas: 10
    # @schema type:[null,integer]
    # -- The target average CPU utilization percentage.
    targetCPU: null
    # @schema type:[null,integer]
    # -- The target average memory utilization percentage.
    targetMemory: null
    # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.autoscaling.v2.MetricSpec
    # -- ([]MetricSpec) Advanced scaling based on custom metrics exposed by Zitadel.
    # To use these for scaling, you MUST have a metrics server
    # (e.g., Prometheus) and a metrics adapter (e.g., prometheus-adapter)
    # running in your cluster.
    # Ref: https://github.com/kubernetes-sigs/prometheus-adapter
    metrics: []
    # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.autoscaling.v2.HorizontalPodAutoscalerBehavior
    # -- (HorizontalPodAutoscalerBehavior) Configures the scaling behavior for scaling up and down.
    # Use this to control how quickly the HPA scales pods in response to metric changes.
    # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#configurable-scaling-behavior
    behavior: {}

# -- Number of ZITADEL pod replicas. While a single replica is fine for testing,
# production environments should use 3 or more to prevent downtime during
# rolling updates, node failures, and ensure high availability.
replicaCount: 1

# Container image configuration for the main ZITADEL application.
image:
  # -- Docker image repository for ZITADEL. The default uses GitHub Container
  # Registry. Change this if using a private registry or mirror.
  repository: ghcr.io/zitadel/zitadel
  # -- Image pull policy. Use "Always" for mutable tags like "latest", or
  # "IfNotPresent" for immutable version tags to reduce network traffic.
  pullPolicy: IfNotPresent
  # -- Image tag. Defaults to the chart's appVersion if not specified. Use a
  # specific version tag (e.g., "v2.45.0") for production deployments to
  # ensure reproducibility and controlled upgrades.
  tag: ""

# -- Global container registry override for tool images (e.g., wait4x, kubectl).
# When set, this registry is prepended to tool image repositories for
# compatibility with CRI-O v1.34+ which enforces fully qualified image names.
# If left empty, defaults to "docker.io".
imageRegistry: ""
# @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.LocalObjectReference
# -- ([]LocalObjectReference) References to secrets containing Docker registry credentials
# for pulling private ZITADEL images. Each entry should be the name of an existing secret
# of type kubernetes.io/dockerconfigjson.
# Example:
#   imagePullSecrets:
#     - name: my-registry-secret
imagePullSecrets: []
# -- Override the "zitadel" portion of resource names. Useful when the default
# naming would conflict with existing resources or when deploying multiple
# instances with different configurations.
nameOverride: ""
# -- Completely override the generated resource names (release-name + chart-name).
# Takes precedence over nameOverride. Use this when you need full control over
# resource naming, such as when migrating from another chart.
fullnameOverride: ""

# -- (map[string]string) Annotations to add to the ZITADEL Deployment resource.
# Use this for integration with tools like ArgoCD, Flux, or external monitoring systems.
annotations: {}

# ConfigMap configuration for ZITADEL's runtime configuration.
configMap:
  # -- (map[string]string) Annotations for the ZITADEL ConfigMap. The default Helm hooks
  # ensure the ConfigMap is created before the deployment and recreated on upgrades to
  # pick up configuration changes.
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

# ServiceAccount configuration for ZITADEL pods. The service account controls
# what Kubernetes API permissions the pods have.
serviceAccount:
  # -- Whether to create a dedicated service account for ZITADEL. Set to false
  # if you want to use an existing service account or the default account.
  create: true
  # -- (map[string]string) Annotations to add to the service account. The default Helm
  # hooks ensure the service account exists before pods are created. Add annotations
  # here for cloud provider integrations (e.g., AWS IAM roles, GCP Workload Identity).
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"
  # -- The name of the service account to use. If not set and create is true, a
  # name is generated using the fullname template. Set this to use a specific
  # existing service account when create is false.
  name: ""

# -- (map[string]string) Annotations to add to ZITADEL pods. Use this for integrations
# like Prometheus scraping, Istio sidecar injection, Vault agent injection, or any
# other annotation-based configuration.
# Example:
#   podAnnotations:
#     prometheus.io/scrape: "true"
#     sidecar.istio.io/inject: "true"
podAnnotations: {}

# -- (map[string]string) Additional labels to add to ZITADEL pods beyond the standard
# Helm labels. Useful for organizing pods with custom label selectors, network policies,
# or pod security policies.
podAdditionalLabels: {}

# Chart-wide pod security context applied to all pods (ZITADEL, Login UI, jobs)
# by default. Individual components can override these via their own
# podSecurityContext settings (e.g., login.podSecurityContext).
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
# @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.PodSecurityContext
podSecurityContext:
  # -- Require containers to run as a non-root user. This is a security best
  # practice that prevents privilege escalation attacks.
  runAsNonRoot: true
  # -- User ID to run the container processes. 1000 is a common non-root UID
  # that matches the ZITADEL container's default user.
  runAsUser: 1000
  # -- Group ID for volume ownership and file creation. Files created in
  # mounted volumes will be owned by this group.
  fsGroup: 1000

# Chart-wide container security context applied to all containers by default.
# Individual components can override these via their own securityContext
# settings (e.g., login.securityContext).
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
# @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.SecurityContext
securityContext:
  # -- Require the container to run as a non-root user.
  runAsNonRoot: true
  # -- User ID to run the container process.
  runAsUser: 1000
  # -- Mount the container's root filesystem as read-only. This prevents
  # malicious processes from writing to the filesystem and is a security
  # best practice for immutable containers.
  readOnlyRootFilesystem: true
  # -- Prevent the container from running in privileged mode. Privileged
  # containers have access to all host devices and capabilities.
  privileged: false

# @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.EnvVar
# -- ([]EnvVar) Additional environment variables for the ZITADEL container. Use this to pass
# configuration that isn't available through configmapConfig or secretConfig,
# or to inject values from other Kubernetes resources like ConfigMaps or Secrets.
# ZITADEL environment variables follow the pattern ZITADEL_<SECTION>_<KEY>.
# Ref: https://zitadel.com/docs/self-hosting/manage/configure#configure-by-environment-variables
env: []
  # Example: Set database host from an external secret managed by an operator
  # - name: ZITADEL_DATABASE_POSTGRES_HOST
  #   valueFrom:
  #     secretKeyRef:
  #       name: postgres-pguser-postgres
  #       key: host
  # Example: Set a static configuration value
  # - name: ZITADEL_LOG_LEVEL
  #   value: "debug"

# -- Name of a Kubernetes Secret containing environment variables to inject into
# the ZITADEL container. All key-value pairs in the secret will be available
# as environment variables. This is useful for managing multiple ZITADEL
# configuration values in a single secret, especially when using external
# secret management tools like External Secrets Operator or Sealed Secrets.
# Ref: https://zitadel.com/docs/self-hosting/manage/configure#configure-by-environment-variables
envVarsSecret: ""

# Kubernetes Service configuration for the main ZITADEL application. This
# service exposes ZITADEL's HTTP/2 API which serves both REST and gRPC traffic.
service:
  # -- Service type. Use "ClusterIP" for internal-only access (typical when using
  # an ingress controller), "NodePort" for direct node-level access, or
  # "LoadBalancer" for cloud provider load balancers.
  type: ClusterIP
  # -- Fixed cluster IP address for ClusterIP services. Leave empty for automatic
  # assignment by Kubernetes. Only applicable when type is "ClusterIP". Setting
  # a fixed IP is useful when other services need a stable endpoint.
  clusterIP: ""
  # -- Traffic policy for LoadBalancer and NodePort services. "Cluster" distributes
  # traffic across all nodes (default), "Local" only routes to nodes with pods,
  # preserving client source IP but potentially causing uneven load distribution.
  externalTrafficPolicy: ""
  # -- Port number the service exposes. Clients and ingress controllers connect to
  # this port. ZITADEL uses 8080 by default for its HTTP/2 server.
  port: 8080
  # -- Protocol identifier used in port naming. ZITADEL uses HTTP/2 (h2c) for both
  # REST API and gRPC traffic on the same port.
  protocol: http2
  # -- Application protocol hint for ingress controllers and service meshes.
  # "kubernetes.io/h2c" indicates HTTP/2 over cleartext (without TLS).
  appProtocol: kubernetes.io/h2c
  # -- (map[string]string) Annotations to add to the Service resource. The default
  # annotation tells Traefik to use HTTP/2 when communicating with ZITADEL backends.
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  # -- (map[string]string) Labels to add to the Service resource. Use for organizing
  # services or matching service selectors in network policies.
  labels: {}
  # -- HTTP scheme for health checks and internal communication. Use "HTTP" when
  # TLS termination happens at the ingress/load balancer, or "HTTPS" when
  # ZITADEL is configured with internal TLS.
  scheme: HTTP

# Ingress configuration for exposing the main ZITADEL service. This allows
# external traffic to reach the ZITADEL API and gRPC endpoints. It can be
# configured to work with various ingress controllers like NGINX, Traefik, or
# AWS ALB.
ingress:
  # -- If true, creates an Ingress resource for the ZITADEL service.
  enabled: false
  # -- A chart-specific setting to enable logic for different controllers.
  # Use "aws" to generate AWS ALB-specific annotations and resources.
  controller: generic
  # -- The name of the IngressClass resource to use for this Ingress.
  # Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class
  className: ""
  # -- (map[string]string) Annotations to apply to the Ingress resource. The default
  # annotation is for NGINX to correctly handle gRPC traffic.
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
  # @schema
  # type: array
  # items:
  #   type: object
  #   properties:
  #     paths:
  #       type: array
  #       items:
  #         type: object
  #         properties:
  #           path:
  #             type: string
  #           pathType:
  #             type: string
  #             enum: [Exact, Prefix, ImplementationSpecific]
  #         required: [pathType]
  # @schema
  # -- A list of host rules for the Ingress. Each host can have multiple paths.
  hosts:
    - paths:
        - path: /
          pathType: Prefix
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.networking.v1.IngressTLS
  # -- ([]IngressTLS) TLS configuration for the Ingress. This allows you to secure the endpoint
  # with HTTPS by referencing a secret that contains the TLS certificate and key.
  tls: []

# @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.ResourceRequirements
# -- (ResourceRequirements) CPU and memory resource requests and limits for the ZITADEL container.
# Setting appropriate resources ensures predictable performance and prevents
# resource starvation. Requests affect scheduling; limits enforce caps.
# Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
resources: {}
  # Example configuration for a production deployment:
  # requests:
  #   cpu: 500m
  #   memory: 512Mi
  # limits:
  #   cpu: 2000m
  #   memory: 2Gi

# -- (map[string]string) Node labels for pod assignment. Pods will only be scheduled
# on nodes that have all the specified labels. Use this to target specific node pools
# (e.g., high-memory nodes, nodes in specific zones).
# Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
nodeSelector: {}
  # Example: Schedule only on nodes labeled for production workloads
  # workload-type: production

# @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Toleration
# -- ([]Toleration) Tolerations allow pods to be scheduled on nodes with matching taints.
# Taints are used to repel pods from nodes; tolerations allow exceptions.
# Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
tolerations: []
  # Example: Tolerate nodes dedicated to ZITADEL
  # - key: "dedicated"
  #   operator: "Equal"
  #   value: "zitadel"
  #   effect: "NoSchedule"

# @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Affinity
# -- (Affinity) Affinity rules for pod scheduling. Use for advanced pod placement
# strategies like co-locating pods on the same node (pod affinity), spreading pods
# across zones (pod anti-affinity), or preferring certain nodes (node affinity).
# Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
affinity: {}
  # Example: Spread ZITADEL pods across availability zones
  # podAntiAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       podAffinityTerm:
  #         labelSelector:
  #           matchLabels:
  #             app.kubernetes.io/name: zitadel
  #         topologyKey: topology.kubernetes.io/zone

# @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.TopologySpreadConstraint
# -- ([]TopologySpreadConstraint) Topology spread constraints control how pods are
# distributed across topology domains (e.g., zones, nodes, regions) for high availability.
# Unlike affinity, these constraints provide more granular control over pod distribution.
# Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
topologySpreadConstraints: []
  # Example: Ensure even distribution across zones with at most 1 pod difference
  # - maxSkew: 1
  #   topologyKey: topology.kubernetes.io/zone
  #   whenUnsatisfiable: DoNotSchedule
  #   labelSelector:
  #     matchLabels:
  #       app.kubernetes.io/name: zitadel

# Init Job configuration for database initialization. This Kubernetes Job runs
# as a Helm pre-install/pre-upgrade hook to prepare the database before ZITADEL
# starts. It creates the database schema, user, and grants necessary permissions.
initJob:
  # -- Enable or disable the init job. Set to false after the initial installation
  # if you want to manage database initialization externally or if no database
  # changes are expected during upgrades.
  enabled: true
  # -- (map[string]string) Annotations for the init job. The Helm hooks ensure this job
  # runs before the main deployment and is recreated on each upgrade.
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "1"
  # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.ResourceRequirements
  # -- (ResourceRequirements) CPU and memory resource requests and limits for the init job container.
  # The init job typically requires minimal resources as it only runs SQL
  # commands against the database.
  resources: {}
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
    # limits:
    #   cpu: 500m
    #   memory: 256Mi
  # -- Number of retries before marking the init job as failed. Increase this if
  # the database might take longer to become available.
  backoffLimit: 5
  # -- Maximum time in seconds for the init job to complete. The job is terminated
  # if it exceeds this deadline, regardless of backoffLimit.
  activeDeadlineSeconds: 300
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # -- ([]Container) Init containers to run before the main init container. Useful for waiting
  # on additional dependencies or performing pre-initialization tasks.
  initContainers: []
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # -- ([]Container) Sidecar containers to run alongside the init container. Useful for logging,
  # proxies (e.g., cloud-sql-proxy), or other supporting services.
  extraContainers: []
  # -- (map[string]string) Additional annotations to add to init job pods.
  podAnnotations: {}
  # -- (map[string]string) Additional labels to add to init job pods.
  podAdditionalLabels: {}
  # Specifies which initialization command to run. Each command performs a
  # subset of the full initialization process:
  #   "": Full initialization - creates database, user, grants, and ZITADEL schema
  #   "database": Only create the database
  #   "user": Only create the database user
  #   "grant": Only set ALL grants to the user
  #   "zitadel": Only initialize ZITADEL internals (skips database and user creation)
  # Use specific commands when you need fine-grained control over initialization,
  # such as when the database is managed by an external operator.
  command: ""

# Cleanup Job configuration for removing imperatively created resources on
# helm uninstall. The setup job creates secrets using kubectl that aren't
# tracked by Helm's lifecycle, causing them to persist after chart removal.
# This job runs as a post-delete hook to clean up those orphaned resources.
cleanupJob:
  # -- Enable the cleanup job to remove secrets created by the setup job.
  # Set to false if you want to preserve secrets across reinstalls.
  enabled: true
  # -- (map[string]string) Annotations for the cleanup job. The post-delete hook ensures
  # this runs on helm uninstall, and the delete policy removes the job after completion.
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-1"
  # -- Number of retries before marking the cleanup job as failed.
  backoffLimit: 3
  # -- Maximum time in seconds for the cleanup job to complete.
  # After this deadline, the job is terminated even if still running.
  activeDeadlineSeconds: 60
  # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.ResourceRequirements
  # -- (ResourceRequirements) Resource limits and requests for the cleanup job container.
  # Keep minimal as this job only runs kubectl delete commands.
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 50m
    #   memory: 64Mi
  # -- (map[string]string) Additional annotations to add to cleanup job pods.
  podAnnotations: {}
  # -- (map[string]string) Additional labels to add to cleanup job pods.
  podAdditionalLabels: {}

# Setup Job configuration for ZITADEL application setup. This Kubernetes Job
# runs as a Helm pre-install/pre-upgrade hook after the init job to configure
# ZITADEL itself. It creates the first organization, admin user, machine users,
# and generates authentication credentials stored as Kubernetes Secrets.
setupJob:
  # -- (map[string]string) Annotations for the setup job. The Helm hooks ensure this job
  # runs after the init job (weight "2" > "1") and before the main deployment.
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "2"
  # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.ResourceRequirements
  # -- (ResourceRequirements) CPU and memory resource requests and limits for the setup job container.
  # The setup job performs more work than init, including generating keys and
  # creating initial data.
  resources: {}
    # requests:
    #   cpu: 200m
    #   memory: 256Mi
    # limits:
    #   cpu: 1000m
    #   memory: 512Mi
  # -- Number of retries before marking the setup job as failed.
  backoffLimit: 5
  # -- Maximum time in seconds for the setup job to complete. The job is terminated
  # if it exceeds this deadline. Increase this for slow environments.
  activeDeadlineSeconds: 300
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # -- ([]Container) Init containers to run before the main setup container. Useful for waiting
  # on additional dependencies or performing pre-setup tasks.
  initContainers: []
  # @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
  # -- ([]Container) Sidecar containers to run alongside the setup container. Useful for logging,
  # proxies (e.g., cloud-sql-proxy), or other supporting services.
  extraContainers: []
  # -- (map[string]string) Additional annotations to add to setup job pods.
  podAnnotations: {}
  # -- (map[string]string) Additional labels to add to setup job pods.
  podAdditionalLabels: {}
  # -- ([]string) Additional command-line arguments to pass to the ZITADEL setup command.
  # The default enables projection initialization for better startup performance.
  additionalArgs:
    - "--init-projections=true"
  # Configuration for the sidecar container that writes machine keys and PATs
  # to Kubernetes Secrets. This container runs alongside the setup container
  # and uses kubectl to create secrets from the generated credentials.
  machinekeyWriter:
    # Image configuration for the machinekey writer container.
    image:
      # -- Override the default kubectl image repository. Leave empty to use the
      # value from tools.kubectl.image.repository.
      repository: ""
      # -- Override the default kubectl image tag. Leave empty to use the value
      # from tools.kubectl.image.tag (which defaults to cluster version).
      tag: ""
    # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.ResourceRequirements
    # -- (ResourceRequirements) CPU and memory resource requests and limits for the machinekey writer
    # container. This container only runs kubectl commands and needs minimal
    # resources.
    resources: {}

# Readiness probe configuration for ZITADEL. The readiness probe determines
# when a pod is ready to receive traffic. Failed probes remove the pod from
# service endpoints, preventing traffic from being routed to unhealthy pods.
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
readinessProbe:
  # -- Enable or disable the readiness probe.
  enabled: true
  # -- Seconds to wait before starting readiness checks after container start.
  # Set higher if ZITADEL needs time to initialize before accepting traffic.
  initialDelaySeconds: 0
  # -- How often (in seconds) to perform the readiness check.
  periodSeconds: 5
  # -- Number of consecutive failures before marking the pod as not ready.
  failureThreshold: 3

# Liveness probe configuration for ZITADEL. The liveness probe determines if
# a container is still running properly. Failed probes cause the container to
# be restarted, which can help recover from deadlocks or stuck states.
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  # -- Enable or disable the liveness probe.
  enabled: true
  # -- Seconds to wait before starting liveness checks after container start.
  initialDelaySeconds: 0
  # -- How often (in seconds) to perform the liveness check.
  periodSeconds: 5
  # -- Number of consecutive failures before restarting the container.
  failureThreshold: 3

# Startup probe configuration for ZITADEL. The startup probe runs before
# liveness and readiness probes begin, allowing slow-starting containers to
# fully initialize. This is especially useful for ZITADEL which may need time
# to complete database migrations on first start.
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
startupProbe:
  # -- Enable or disable the startup probe. When enabled, liveness and readiness
  # probes are disabled until the startup probe succeeds.
  enabled: true
  # -- How often (in seconds) to perform the startup check.
  periodSeconds: 1
  # -- Number of consecutive failures before marking startup as failed and
  # restarting the container. With periodSeconds=1 and failureThreshold=30,
  # the container has 30 seconds to start.
  failureThreshold: 30

# Metrics configuration for ZITADEL monitoring. When enabled, ZITADEL exposes
# Prometheus-compatible metrics at /debug/metrics on the main service port.
# These metrics include Go runtime statistics, gRPC metrics, and ZITADEL-specific
# operational metrics like projection event counts.
metrics:
  # -- Enable metrics scraping annotations on ZITADEL pods. When true, adds
  # prometheus.io/* annotations that enable automatic discovery by Prometheus.
  enabled: false
  # ServiceMonitor configuration for Prometheus Operator. A ServiceMonitor is a
  # custom resource that tells Prometheus Operator how to scrape metrics from
  # ZITADEL. Requires the Prometheus Operator to be installed in the cluster.
  # Ref: https://github.com/prometheus-operator/prometheus-operator
  serviceMonitor:
    # Create a ServiceMonitor resource for Prometheus Operator integration.
    # The Prometheus community Helm chart (kube-prometheus-stack) installs
    # this operator. To allow discovery across all namespaces, set:
    # prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
    # Example query: sum({__name__="projection_events_processed_total"})
    enabled: false
    # -- If true, use metric labels from ZITADEL instead of relabeling them to
    # match Prometheus conventions.
    honorLabels: false
    # -- If true, preserve original scrape timestamps from ZITADEL instead of
    # using Prometheus server time.
    honorTimestamps: true
    # @schema type:[null,string]
    # -- Namespace where the ServiceMonitor should be created. If null, uses the
    # release namespace. Set this if Prometheus watches a specific namespace.
    namespace: null
    # -- (map[string]string) Additional labels to add to the ServiceMonitor. Use this
    # to match Prometheus Operator's serviceMonitorSelector if configured.
    additionalLabels: {}
    # @schema type:[null,string]
    # -- How often Prometheus should scrape metrics from ZITADEL. If null, uses
    # Prometheus's default scrape interval (typically 30s).
    scrapeInterval: null
    # @schema type:[null,string]
    # -- Timeout for scrape requests. If null, uses Prometheus's default timeout.
    # Should be less than scrapeInterval.
    scrapeTimeout: null
    # -- ([]RelabelConfig) Relabeling rules applied before ingestion. Use to modify, filter, or
    # drop labels before metrics are stored.
    relabellings: []
    # -- ([]RelabelConfig) Relabeling rules applied to individual metrics. Use to rename metrics,
    # drop expensive metrics, or modify metric labels.
    metricRelabellings: []
    # @schema type:[null,string]
    # -- HTTP scheme to use for scraping. Set to "https" if ZITADEL has internal
    # TLS enabled. If null, defaults to "http".
    scheme: null
    # -- (TLSConfig) TLS configuration for scraping HTTPS endpoints. Configure this if ZITADEL
    # has internal TLS enabled and you need to verify certificates.
    tlsConfig: {}
    # @schema type:[null,string]
    # -- HTTP proxy URL for scraping. Use if Prometheus needs to access ZITADEL
    # through a proxy.
    proxyUrl: null

# Pod Disruption Budget configuration for the main ZITADEL deployment.
# Ensures high availability by limiting the number of pods that can be
# simultaneously unavailable during voluntary disruptions (e.g., node drains,
# rolling updates). Either minAvailable or maxUnavailable can be set, but not
# both. Values can be an integer (e.g., 1) or a percentage (e.g., "50%").
pdb:
  # -- Enable or disable the Pod Disruption Budget for ZITADEL pods
  enabled: false
  # @schema type:[null,integer,string]
  # -- Minimum number of pods that must remain available during disruptions.
  # Cannot be used together with maxUnavailable.
  minAvailable: null
  # @schema type:[null,integer,string]
  # -- Maximum number of pods that can be unavailable during disruptions.
  # Cannot be used together with minAvailable.
  maxUnavailable: null
  # -- (map[string]string) Additional annotations to apply to the Pod Disruption Budget resource.
  annotations: {}

# @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Container
# -- ([]Container) Sidecar containers to run alongside the main ZITADEL container in the
# Deployment pod. Use this for logging agents, monitoring sidecars, service
# meshes, or database proxies (e.g., cloud-sql-proxy for Google Cloud SQL).
# These containers share the pod's network namespace and can access the same
# volumes as the main container.
extraContainers: []
  # Example: Add a cloud-sql-proxy sidecar for Google Cloud SQL
  # - name: cloud-sql-proxy
  #   image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.14.1
  #   command: ["/cloud-sql-proxy"]
  #   args:
  #     - "my-project:my-region:my-instance"
  #     - "--port=5432"
  #   securityContext:
  #     runAsNonRoot: true

# @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.Volume
# -- ([]Volume) Additional volumes to add to ZITADEL pods. These volumes can be referenced
# by extraVolumeMounts to make data available to the ZITADEL container or
# sidecar containers. Supports all Kubernetes volume types: secrets, configMaps,
# persistentVolumeClaims, emptyDir, hostPath, etc.
extraVolumes: []
  # Example: Mount a custom CA certificate from a secret
  # - name: ca-certs
  #   secret:
  #     defaultMode: 420
  #     secretName: ca-certs
  # Example: Mount configuration from a ConfigMap
  # - name: custom-config
  #   configMap:
  #     name: my-custom-config

# @schema itemRef: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.VolumeMount
# -- ([]VolumeMount) Additional volume mounts for the main ZITADEL container. Use this to mount
# volumes defined in extraVolumes into the container filesystem. Common use
# cases include mounting custom CA certificates, configuration files, or
# shared data between containers.
extraVolumeMounts: []
  # Example: Mount a custom CA certificate for outbound TLS connections
  # - name: ca-certs
  #   mountPath: /etc/ssl/certs/myca.pem
  #   subPath: myca.pem
  #   readOnly: true

# -- ([]object) Additional Kubernetes manifests to deploy alongside the chart. This allows
# you to include custom resources without creating a separate chart. Supports
# Helm templating syntax including .Release, .Values, and template functions.
# Use this for secrets, configmaps, network policies, or any other resources
# that ZITADEL depends on.
extraManifests: []
  # Example: Create a secret with ZITADEL configuration
  # - apiVersion: v1
  #   kind: Secret
  #   metadata:
  #     name: {{ include "zitadel.fullname" . }}-my-secret
  #   stringData:
  #     key: value
  #   type: Opaque
  # Example: Create a NetworkPolicy to restrict ZITADEL traffic
  # - apiVersion: networking.k8s.io/v1
  #   kind: NetworkPolicy
  #   metadata:
  #     name: {{ include "zitadel.fullname" . }}-network-policy
  #   spec:
  #     podSelector:
  #       matchLabels:
  #         app.kubernetes.io/name: zitadel
  #     policyTypes:
  #       - Ingress

# Configuration for helper tools used by init containers and jobs. These images
# are used by components such as wait-for-zitadel, wait-for-postgres, and the
# setup and cleanup jobs. Each tool follows the standard image configuration
# pattern with registry, repository, tag, pull policy, and pull secrets.
tools:
  # Configuration for the wait4x image used for readiness and dependency checks
  # in init containers. Values are intentionally left empty and should be set by
  # the user when overriding this configuration.
  wait4x:
    image:
      # -- The name of the image repository that contains the wait4x image. The
      # chart automatically prepends the registry (docker.io by default) for
      # compatibility with CRI-O v1.34+ which enforces fully qualified names.
      repository: "wait4x/wait4x"
      # -- The image tag to use for the wait4x image. Leave empty to require the
      # user to set a specific version explicitly.
      tag: "3.6"
      # -- The pull policy for the wait4x image. If left empty, the chart defaults
      # to the Kubernetes default pull policy for the given tag.
      pullPolicy: ""
    # @schema $ref: $k8s/_definitions.json#/definitions/io.k8s.api.core.v1.ResourceRequirements
    # -- (ResourceRequirements) CPU and memory resource requests and limits for wait4x init containers.
    # These resources apply to all init containers using the wait4x tool,
    # including wait-for-zitadel and wait-for-postgres. Setting equal requests
    # and limits enables the "Guaranteed" QoS class when combined with resource
    # settings on the main container.
    # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources: {}
      # Example for Guaranteed QoS class:
      # requests:
      #   cpu: 50m
      #   memory: 32Mi
      # limits:
      #   cpu: 50m
      #   memory: 32Mi

  # Configuration for the kubectl helper image used by init containers and jobs
  # for lightweight Kubernetes API operations. This image is used by the setup
  # job's machinekey containers and the cleanup job.
  kubectl:
    image:
      # -- The name of the image repository that contains the kubectl image. The
      # chart automatically prepends the registry (docker.io by default) for
      # compatibility with CRI-O v1.34+ which enforces fully qualified names.
      repository: "alpine/k8s"
      # -- The image tag to use for the kubectl image. It should be left empty to
      # automatically default to the Kubernetes cluster version
      tag: ""
      # -- The pull policy for the kubectl image. If left empty, Kubernetes applies
      # its default policy depending on whether the tag is mutable or fixed.
      pullPolicy: ""
