# Default values for zitadel.
zitadel:
  # The ZITADEL config under configmapConfig is written to a Kubernetes ConfigMap
  # See all defaults here:
  # https://github.com/zitadel/zitadel/blob/main/cmd/defaults.yaml
  configmapConfig:
    ExternalSecure: true
    Machine:
      Identification:
        Hostname:
          Enabled: true
        Webhook:
          Enabled: false
    FirstInstance:
      Org:
        LoginClient:
          Machine:
            Username: 'login-client'
            Name: 'Automatically Initialized IAM Login Client'
          Pat.ExpirationDate: '2029-01-01T00:00:00Z'

  # The ZITADEL config under secretConfig is written to a Kubernetes Secret
  # See all defaults here:
  # https://github.com/zitadel/zitadel/blob/main/cmd/defaults.yaml
  secretConfig:

  # Annotations set on secretConfig secret
  secretConfigAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # Reference the name of a secret that contains ZITADEL configuration.
  configSecretName:
  # The key under which the ZITADEL configuration is located in the secret.
  configSecretKey: config-yaml

  # ZITADEL uses the masterkey for symmetric encryption.
  # You can generate it for example with tr -dc A-Za-z0-9 </dev/urandom | head -c 32
  masterkey: ""
  # Reference the name of the secret that contains the masterkey. The key should be named "masterkey".
  # Note: Either zitadel.masterkey or zitadel.masterkeySecretName must be set
  masterkeySecretName: ""

  # Number of old ReplicaSets to retain for rollback purposes
  # Set to 0 to not keep any old ReplicaSets
  revisionHistoryLimit: 10

  # Horizontal Pod Autoscaler configuration for scaling on CPU, memory, or custom metrics.
  # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  autoscaling:
    # If true, enables the Horizontal Pod Autoscaler for the Zitadel deployment.
    # This will automatically override the `replicaCount` value.
    enabled: false
    # Optional map of annotations applied to the HPA object.
    annotations: {}
    # The minimum number of pod replicas.
    minReplicas: 3
    # The maximum number of pod replicas.
    maxReplicas: 10
    # The target average CPU utilization percentage.
    targetCPU: null
    # The target average memory utilization percentage.
    targetMemory: null
    # Advanced scaling based on custom metrics exposed by Zitadel.
    # Zitadel exposes standard Go runtime metrics. To use these for scaling,
    # you MUST have a metrics server (e.g., Prometheus) and a metrics adapter
    # (e.g., prometheus-adapter) running in your cluster.
    # Ref: https://github.com/kubernetes-sigs/prometheus-adapter
    metrics: []
    # Example: Scale when the average number of goroutines per pod exceeds 150.
    # The `go_goroutines` metric is a good proxy for concurrent load.
    # You should observe your application's baseline to find a suitable value.
    # - type: Pods
    #   pods:
    #     metric:
    #       # This name must match the metric exposed by the prometheus-adapter.
    #       name: "go_goroutines"
    #     target:
    #       type: AverageValue
    #       averageValue: "150"
    # Configures the scaling behavior for scaling up and down.
    # See: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#configurable-scaling-behavior
    behavior: {}

  # Annotations set on masterkey secret
  masterkeyAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # The CA Certificate needed for establishing secure database connections
  dbSslCaCrt: ""

  # Annotations set on database SSL CA certificate secret
  dbSslCaCrtAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # The Secret containing the CA certificate at key ca.crt needed for establishing secure database connections
  dbSslCaCrtSecret: ""

  # The db admins secret containing the client certificate and key at tls.crt and tls.key needed for establishing secure database connections
  dbSslAdminCrtSecret: ""

  # The db users secret containing the client certificate and key at tls.crt and tls.key needed for establishing secure database connections
  dbSslUserCrtSecret: ""

  # The Secret containing the certificate at key tls.crt and tls.key for listening on HTTPS
  serverSslCrtSecret: ""

  # Generate a self-signed TLS certificate using Helm's native functions.
  # If enabled, this creates a Kubernetes secret containing a self-signed
  # certificate and mounts it to /etc/tls/ in the ZITADEL pod.
  #
  # This is useful for quick demos or test environments without an ingress
  # controller. It is NOT recommended for production use.
  #
  # The certificate will be valid for the `ExternalDomain`, `localhost`,
  # and any host specified in `additionalDnsName`. It can't use dynamic
  # values like the Pod IP, which the previous initContainer method did.
  selfSignedCert:
    # -- If true, enables the generation of the self-signed certificate.
    enabled: false
    # -- An additional DNS name (SAN) to add to the certificate.
    # e.g., "my-zitadel.example.com"
    additionalDnsName: ""

  # Enabling this will create a debug pod that can be used to inspect the ZITADEL configuration and run zitadel commands using the zitadel binary.
  # This is useful for debugging and troubleshooting.
  # After the debug pod is created, you can open a shell within the pod.
  # See more instructions by printing the pods logs using kubectl logs [pod name].
  debug:
    enabled: false
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-weight: "1"
    initContainers: []
    extraContainers: []

  # initContainers allow you to add any init containers you wish to use globally.
  # Additionally, they follow the same structure as extraContainers
  initContainers: []
  # extraContainers allows you to add any sidecar containers you wish to use globally.
  # Currently this is the Zitadel Deployment, Setup Job**, Init Job** and debug_replicaset**  **If Enabled
  extraContainers: []
    # # Example; You wish to deploy a cloud-sql-proxy sidecar to all deployments:
    # - name: cloud-sql-proxy
    #   image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.14.1
    #   command:
    #     - /cloud-sql-proxy
    #   args:
    #     - my-project:my-region:my-instance
    #     - --port=5432
    #     - --auto-iam-authn
    #     - --health-check
    #     - "--http-address=0.0.0.0"
    #   ports:
    #     - containerPort: 5432
    #   startupProbe:
    #     httpGet:
    #       path: /startup
    #       port: 9090
    #     periodSeconds: 1
    #     timeoutSeconds: 5
    #   livenessProbe:
    #     httpGet:
    #       path: /liveness
    #       port: 9090
    #     initialDelaySeconds: 0
    #     periodSeconds: 60
    #     timeoutSeconds: 30
    #     failureThreshold: 5
    #   securityContext:
    #     runAsNonRoot: true
    #     readOnlyRootFilesystem: true
    #     allowPrivilegeEscalation: false
    #   lifecycle:
    #     postStart:
    #       exec:
    #         command: ["/cloud-sql-proxy", "wait"]

login:
  enabled: true
  # Pod Disruption Budget configuration for the main ZITADEL deployment.
  # Ensures high availability by limiting the number of pods that can be
  # simultaneously unavailable during voluntary disruptions (e.g., node drains,
  # rolling updates). Either minAvailable or maxUnavailable can be set, but not
  # both. Values can be an integer (e.g., 1) or a percentage (e.g., "50%").
  pdb:
    # Enable or disable the Pod Disruption Budget for ZITADEL pods
    enabled: false
    # Minimum number of pods that must remain available during disruptions.
    # Cannot be used together with maxUnavailable.
    minAvailable: null
    # Maximum number of pods that can be unavailable during disruptions.
    # Cannot be used together with minAvailable.
    maxUnavailable: null
    # Additional annotations to apply to the Pod Disruption Budget resource
    annotations: {}

  # customConfigmapConfig defaults to this:
  # ZITADEL_SERVICE_USER_TOKEN_FILE="/login-client/pat"
  # ZITADEL_API_URL="http://{{ include "zitadel.fullname" . }}:{{ .Values.service.port }}"
  # CUSTOM_REQUEST_HEADERS="Host:{{ .Values.zitadel.configmapConfig.ExternalDomain }}"
  customConfigmapConfig:
  # To deploy zitadel multiple times in the same namespace, use a loginClientSecretPrefix.
  # To mount it, also change the referenced secretName for the login client to "{loginClientSecretPrefix}login-client"
  loginClientSecretPrefix:
  extraVolumeMounts:
  - name: login-client
    mountPath: /login-client
    readOnly: true
  extraVolumes:
  - name: login-client
    secret:
      defaultMode: 444
      # Adjust this if you want to reference a different secret for the login client PAT.
      secretName: login-client
  # Number of pod replicas. While a single replica is fine for testing,
  # production environments should use 3 or more to prevent downtime during
  # updates and ensure high availability.
  replicaCount: 1
  initContainers: []
  extraContainers: []
  image:
    repository: ghcr.io/zitadel/zitadel-login
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  podAnnotations: {}
  podAdditionalLabels: {}
  # Annotations to add to the configMap
  configMap:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "0"
  readinessProbe:
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 5
    failureThreshold: 3
  livenessProbe:
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 5
    failureThreshold: 3
  startupProbe:
    enabled: false
    periodSeconds: 1
    failureThreshold: 30
  service:
    type: ClusterIP
    # If service type is "ClusterIP", this can optionally be set to a fixed IP address.
    clusterIP: ""
    # If service type is "LoadBalancer", this can optionally be set to either "Cluster" or "Local"
    externalTrafficPolicy: ""
    port: 3000
    protocol: http
    appProtocol: kubernetes.io/http
    annotations: {}
    labels: {}
    scheme: HTTP
  ingress:
    enabled: false
    className: ""
    annotations: {}
    hosts:
      - paths:
          - path: /ui/v2/login
            pathType: Prefix
    tls: []
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "0"
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    readOnlyRootFilesystem: true
    privileged: false
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []
  # Additional environment variables
  env:
    []
    # - name: ZITADEL_DATABASE_POSTGRES_HOST
    #   valueFrom:
    #     secretKeyRef:
    #       name: postgres-pguser-postgres
    #       key: host
  revisionHistoryLimit: 10

  # Horizontal Pod Autoscaler configuration for scaling on CPU, memory, or custom metrics.
  # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  autoscaling:
    # If true, enables the Horizontal Pod Autoscaler for the Zitadel deployment.
    # This will automatically override the `replicaCount` value.
    enabled: false
    # Optional map of annotations applied to the HPA object.
    annotations: {}
    # The minimum number of pod replicas.
    minReplicas: 3
    # The maximum number of pod replicas.
    maxReplicas: 10
    # The target average CPU utilization percentage.
    targetCPU: null
    # The target average memory utilization percentage.
    targetMemory: null
    # Advanced scaling based on custom metrics exposed by Zitadel.
    # To use these for scaling, you MUST have a metrics server
    # (e.g., Prometheus) and a metrics adapter (e.g., prometheus-adapter)
    # running in your cluster.
    # Ref: https://github.com/kubernetes-sigs/prometheus-adapter
    metrics: []
    behavior: {}

# Number of pod replicas. While a single replica is fine for testing,
# production environments should use 3 or more to prevent downtime during
# updates and ensure high availability.
replicaCount: 1

image:
  repository: ghcr.io/zitadel/zitadel
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Annotations to add to the deployment
annotations: {}

# Annotations to add to the configMap
configMap:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podAdditionalLabels: {}

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  readOnlyRootFilesystem: true
  privileged: false

# Additional environment variables
env:
  []
  # - name: ZITADEL_DATABASE_POSTGRES_HOST
  #   valueFrom:
  #     secretKeyRef:
  #       name: postgres-pguser-postgres
  #       key: host

# Additional environment variables from the given secret name
# Zitadel can be configured using environment variables from a secret.
# Reference: https://zitadel.com/docs/self-hosting/manage/configure#configure-by-environment-variables
envVarsSecret: ""

service:
  type: ClusterIP
  # If service type is "ClusterIP", this can optionally be set to a fixed IP address.
  clusterIP: ""
  # If service type is "LoadBalancer", this can optionally be set to either "Cluster" or "Local"
  externalTrafficPolicy: ""
  port: 8080
  protocol: http2
  appProtocol: kubernetes.io/h2c
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels: {}
  scheme: HTTP

ingress:
  enabled: false
  className: ""
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
  hosts:
    - paths:
        - path: /
          pathType: Prefix
  tls: []

resources: {}

nodeSelector: {}

tolerations: []

affinity: {}

topologySpreadConstraints: []

initJob:
  # Once ZITADEL is installed, the initJob can be disabled.
  enabled: true
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "1"
  resources: {}
  backoffLimit: 5
  activeDeadlineSeconds: 300
  initContainers: []
  extraContainers: []
  podAnnotations: {}
  podAdditionalLabels: {}
  # Available init commands :
  # "": initialize ZITADEL instance (without skip anything)
  # database: initialize only the database
  # grant: set ALL grant to user
  # user: initialize only the database user
  # zitadel: initialize ZITADEL internals (skip "create user" and "create database")
  command: ""

setupJob:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "2"
  resources: {}
  backoffLimit: 5
  activeDeadlineSeconds: 300
  initContainers: []
  extraContainers: []
  podAnnotations: {}
  podAdditionalLabels: {}
  additionalArgs:
    - "--init-projections=true"
  machinekeyWriter:
    image:
      repository: alpine/k8s
      tag: ""
    resources: {}

readinessProbe:
  enabled: true
  initialDelaySeconds: 0
  periodSeconds: 5
  failureThreshold: 3

livenessProbe:
  enabled: true
  initialDelaySeconds: 0
  periodSeconds: 5
  failureThreshold: 3

startupProbe:
  enabled: true
  periodSeconds: 1
  failureThreshold: 30

metrics:
  enabled: false
  serviceMonitor:
    # If true, the chart creates a ServiceMonitor that is compatible with Prometheus Operator
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.ServiceMonitor.
    # The Prometheus community Helm chart installs this operator
    # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#kube-prometheus-stack
    # Install the prometheus Helm release with the value prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
    # In the Prometheus UI, query Zitadel metrics, for example:
    # sum({__name__="projection_events_processed_total"})
    enabled: false
    honorLabels: false
    honorTimestamps: true

# Pod Disruption Budget configuration for the main ZITADEL deployment.
# Ensures high availability by limiting the number of pods that can be
# simultaneously unavailable during voluntary disruptions (e.g., node drains,
# rolling updates). Either minAvailable or maxUnavailable can be set, but not
# both. Values can be an integer (e.g., 1) or a percentage (e.g., "50%").
pdb:
  # Enable or disable the Pod Disruption Budget for ZITADEL pods
  enabled: false
  # Minimum number of pods that must remain available during disruptions.
  # Cannot be used together with maxUnavailable.
  minAvailable: null
  # Maximum number of pods that can be unavailable during disruptions.
  # Cannot be used together with minAvailable.
  maxUnavailable: null
  # Additional annotations to apply to the Pod Disruption Budget resource
  annotations: {}

# extraContainers allows you to add any sidecar containers you wish to use in the Zitadel pod.
extraContainers: []

extraVolumes: []
  # - name: ca-certs
  #   secret:
  #     defaultMode: 420
  #     secretName: ca-certs

extraVolumeMounts: []
  # - name: ca-certs
  #   mountPath: /etc/ssl/certs/myca.pem
  #   subPath: myca.pem
  #   readOnly: true

# extraManifests allows you to add your own Kubernetes manifests
# You can use templating logic like {{ .Release.Namespace }} and {{ .Values.replicaCount }} as long as your manifest is a valid YAML
extraManifests: []
  # - apiVersion: v1
  #   kind: Secret
  #   metadata:
  #     name: {{ include "zitadel.fullname" . }}-my-secret
  #   stringData:
  #     key: value
  #   type: Opaque
